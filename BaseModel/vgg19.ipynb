{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6866566,"sourceType":"datasetVersion","datasetId":2664248},{"sourceId":9588424,"sourceType":"datasetVersion","datasetId":5847673}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-10T02:56:29.366535Z","iopub.execute_input":"2024-10-10T02:56:29.366934Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/kaggle/input/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n/kaggle/input/models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n/kaggle/input/infrared-solar-modules/LICENSE\n/kaggle/input/infrared-solar-modules/README.md\n/kaggle/input/infrared-solar-modules/2020-02-14_InfraredSolarModules/InfraredSolarModules/module_metadata.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2 as cv\nimport random\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, BatchNormalization, MaxPool2D, Dropout\nfrom tensorflow.keras.models import Sequential\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/infrared-solar-modules/2020-02-14_InfraredSolarModules/InfraredSolarModules/module_metadata.json', 'r') as file:\n    data = json.load(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data).T.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_images(dataframe, base_path=\"/kaggle/input/infrared-solar-modules/2020-02-14_InfraredSolarModules/InfraredSolarModules/\"):\n    images = []\n    labels = []\n    i = 0\n    for idx, row in dataframe.iterrows():\n        i+=1\n        img_path = base_path + row['image_filepath']\n        img = Image.open(img_path).convert('RGB')\n        img = img.resize((128, 128))  # Resize the image\n        img_array = img_to_array(img)\n        img_array = img_array / 255.0  # Normalize to [0, 1]\n        images.append(img_array)\n        labels.append(row['anomaly_class'])\n        #print(f'{i} Done out of {dataframe.shape[0]}')     # enable if you need to check the itrs of reading\n    return np.array(images), np.array(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into features and target || Convert labels to categorical codes\nX = df['image_filepath']\ny = pd.Categorical(df['anomaly_class']).codes \n\nprint(len(X))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into training (60%), validation (20%), and test (20%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Series to DataFrame to reuse the load_images function\ntrain_df = pd.DataFrame({'image_filepath': X_train, 'anomaly_class': y_train})\nval_df = pd.DataFrame({'image_filepath': X_val, 'anomaly_class': y_val})\ntest_df = pd.DataFrame({'image_filepath': X_test, 'anomaly_class': y_test})\n\nprint(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess images\ntrain_images, train_labels = load_images(train_df)\nprint(\"Train Read Done..!\")\nval_images, val_labels = load_images(val_df)\nprint(\"Validation Read Done..!\")\ntest_images, test_labels = load_images(test_df)\nprint(\"Test Read Done..!\")","metadata":{"execution":{"iopub.status.idle":"2024-10-10T02:59:06.787724Z","shell.execute_reply.started":"2024-10-10T02:57:07.916533Z","shell.execute_reply":"2024-10-10T02:59:06.786760Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Train Read Done..!\nValidation Read Done..!\nTest Read Done..!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Optionally, convert labels to categorical format if using categorical crossentropy\ntrain_labels = to_categorical(train_labels)\nval_labels = to_categorical(val_labels)\ntest_labels = to_categorical(test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:59:06.788865Z","iopub.execute_input":"2024-10-10T02:59:06.789190Z","iopub.status.idle":"2024-10-10T02:59:06.827714Z","shell.execute_reply.started":"2024-10-10T02:59:06.789156Z","shell.execute_reply":"2024-10-10T02:59:06.826981Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import cv2 as cv \nimport numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import *\nfrom keras.layers import Bidirectional\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tensorflow.keras.utils import to_categorical\nimport itertools\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers, models, callbacks\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.applications import VGG16, VGG19,ResNet50,ResNet101, ResNet152, InceptionV3, MobileNet, DenseNet121, DenseNet169, DenseNet201, NASNetMobile, Xception\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:59:06.828799Z","iopub.execute_input":"2024-10-10T02:59:06.829113Z","iopub.status.idle":"2024-10-10T02:59:06.847943Z","shell.execute_reply.started":"2024-10-10T02:59:06.829079Z","shell.execute_reply":"2024-10-10T02:59:06.847150Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom tensorflow.keras.utils import to_categorical\nimport cv2 as cv\nimport tensorflow as tf\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:59:06.849187Z","iopub.execute_input":"2024-10-10T02:59:06.849489Z","iopub.status.idle":"2024-10-10T02:59:06.857959Z","shell.execute_reply.started":"2024-10-10T02:59:06.849456Z","shell.execute_reply":"2024-10-10T02:59:06.857096Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n# Custom Callback for logging after each epoch\nclass EpochLogger(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Epoch {epoch + 1}: Train Accuracy: {logs['accuracy']:.4f}, Train Loss: {logs['loss']:.4f}\")\n\n# Function to reshape data for uniform input\ndef reshape_data(data, new_shape):\n    reshaped_data = np.zeros((data.shape[0],) + new_shape)\n    for i in range(data.shape[0]):\n        reshaped_data[i] = cv.resize(data[i], new_shape[:2])  # Resize images to (height, width)\n    return reshaped_data\n\n# Function to create a CNN model\ndef create_cnn_model(input_shape):\n    model = Sequential([\n        Input(shape=input_shape),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(12, activation='softmax')  # 12 classes for your dataset\n    ])\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Base model loading paths\n\ndensenet169_weights_path = '/kaggle/input/models/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#densenet201_weights_path = '/kaggle/input/model2/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# Load the base models (for feature extraction, no fine-tuning)\nbase_models = [\n   \n    VGG19(weights=densenet169_weights_path , include_top=False, input_shape=(224, 224, 3)),\n]\n\n# Hyperparameters for training\nlearning_rates = [0.001]\nepochs_list = [80]\ninput_shape = (128, 128, 3)\nresults = []\n\n# Data augmentation setup\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Augmented training generator (assume `train_images` and `train_labels` are preloaded)\ntrain_generator = datagen.flow(train_images, train_labels, batch_size=32)\n\n# Original dataset size\nN_train = train_images.shape[0]  # Number of training images\nbatch_size = 32  # Your defined batch size\n\n# Calculate batches per epoch\nbatches_per_epoch = N_train // batch_size\n\n# Total augmented images per epoch\naugmented_images_per_epoch = batches_per_epoch * batch_size\nprint(f\"Augmented images generated per epoch: {augmented_images_per_epoch}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:59:06.861574Z","iopub.execute_input":"2024-10-10T02:59:06.861886Z","iopub.status.idle":"2024-10-10T02:59:08.297373Z","shell.execute_reply.started":"2024-10-10T02:59:06.861855Z","shell.execute_reply":"2024-10-10T02:59:08.296471Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Augmented images generated per epoch: 12000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Iterate over each base model, learning rate, and epoch configuration\nfor base_model in base_models:\n    print(f\"Using base model: {base_model.name}\")\n    for lr in learning_rates:\n        print(f\"Learning rate: {lr}\")\n        for epochs in epochs_list:\n            print(f\"Training for {epochs} epochs...\")\n\n            # Create the CNN model (no fine-tuning, just feature extraction)\n            model = create_cnn_model(input_shape=(128, 128, 3))\n            print(\"CNN model created successfully.\")\n\n            # Reshape the data\n            reshaped_x_train = reshape_data(train_images, (128, 128, 3))  # Keep data in 128x128\n            reshaped_x_test = reshape_data(test_images, (128, 128, 3))\n            print(f\"Data reshaped to {reshaped_x_train.shape}.\")\n\n            # Show the size of the augmented image for debugging\n            sample_augmented_image = next(train_generator)[0][0]\n            print(f\"Sample augmented image shape: {sample_augmented_image.shape}\")\n\n            # Train the model using augmented data\n            history = model.fit(train_generator, epochs=epochs, validation_data=(reshaped_x_test, test_labels),\n                                callbacks=[EpochLogger()], verbose=0)\n\n            # Evaluate the model on the test set\n            test_loss, test_accuracy = model.evaluate(reshaped_x_test, test_labels, verbose=0)\n            print(f\"Model trained successfully. Test accuracy: {test_accuracy:.4f}\")\n            \n            # Save the results\n            results.append([base_model.name, epochs, lr, test_accuracy])\n\n# After all models have been trained\nprint(\"Training complete. Saving results to Excel...\")\n\n# Save results to Excel\ndf = pd.DataFrame(results, columns=['Model', 'Epochs', 'Learning Rate', 'Test Accuracy'])\ndf.to_excel('model_accuracies_with_augmentation.xlsx', index=False)\nprint(\"Results saved successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:59:08.298544Z","iopub.execute_input":"2024-10-10T02:59:08.298868Z","iopub.status.idle":"2024-10-10T04:11:03.755165Z","shell.execute_reply.started":"2024-10-10T02:59:08.298833Z","shell.execute_reply":"2024-10-10T04:11:03.754257Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Using base model: vgg19\nLearning rate: 0.001\nTraining for 80 epochs...\nCNN model created successfully.\nData reshaped to (12000, 128, 128, 3).\nSample augmented image shape: (128, 128, 3)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728529157.172584      96 service.cc:145] XLA service 0x78d6d8004dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1728529157.172660      96 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1728529159.793010      96 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Accuracy: 0.4977, Train Loss: 1.8442\nEpoch 2: Train Accuracy: 0.4993, Train Loss: 1.7481\nEpoch 3: Train Accuracy: 0.5002, Train Loss: 1.7267\nEpoch 4: Train Accuracy: 0.5005, Train Loss: 1.7130\nEpoch 5: Train Accuracy: 0.5016, Train Loss: 1.6972\nEpoch 6: Train Accuracy: 0.5013, Train Loss: 1.6917\nEpoch 7: Train Accuracy: 0.5032, Train Loss: 1.6788\nEpoch 8: Train Accuracy: 0.5002, Train Loss: 1.6676\nEpoch 9: Train Accuracy: 0.5048, Train Loss: 1.6642\nEpoch 10: Train Accuracy: 0.5063, Train Loss: 1.6398\nEpoch 11: Train Accuracy: 0.5123, Train Loss: 1.6182\nEpoch 12: Train Accuracy: 0.5214, Train Loss: 1.5790\nEpoch 13: Train Accuracy: 0.5342, Train Loss: 1.5385\nEpoch 14: Train Accuracy: 0.5413, Train Loss: 1.5206\nEpoch 15: Train Accuracy: 0.5469, Train Loss: 1.4965\nEpoch 16: Train Accuracy: 0.5502, Train Loss: 1.4904\nEpoch 17: Train Accuracy: 0.5579, Train Loss: 1.4628\nEpoch 18: Train Accuracy: 0.5563, Train Loss: 1.4773\nEpoch 19: Train Accuracy: 0.5616, Train Loss: 1.4598\nEpoch 20: Train Accuracy: 0.5620, Train Loss: 1.4477\nEpoch 21: Train Accuracy: 0.5680, Train Loss: 1.4433\nEpoch 22: Train Accuracy: 0.5711, Train Loss: 1.4256\nEpoch 23: Train Accuracy: 0.5732, Train Loss: 1.4183\nEpoch 24: Train Accuracy: 0.5716, Train Loss: 1.4158\nEpoch 25: Train Accuracy: 0.5747, Train Loss: 1.3970\nEpoch 26: Train Accuracy: 0.5804, Train Loss: 1.3877\nEpoch 27: Train Accuracy: 0.5760, Train Loss: 1.4103\nEpoch 28: Train Accuracy: 0.5821, Train Loss: 1.3781\nEpoch 29: Train Accuracy: 0.5882, Train Loss: 1.3763\nEpoch 30: Train Accuracy: 0.5924, Train Loss: 1.3653\nEpoch 31: Train Accuracy: 0.5898, Train Loss: 1.3571\nEpoch 32: Train Accuracy: 0.5914, Train Loss: 1.3566\nEpoch 33: Train Accuracy: 0.5980, Train Loss: 1.3466\nEpoch 34: Train Accuracy: 0.5964, Train Loss: 1.3420\nEpoch 35: Train Accuracy: 0.5969, Train Loss: 1.3385\nEpoch 36: Train Accuracy: 0.5978, Train Loss: 1.3340\nEpoch 37: Train Accuracy: 0.5973, Train Loss: 1.3405\nEpoch 38: Train Accuracy: 0.5992, Train Loss: 1.3287\nEpoch 39: Train Accuracy: 0.6004, Train Loss: 1.3295\nEpoch 40: Train Accuracy: 0.6011, Train Loss: 1.3173\nEpoch 41: Train Accuracy: 0.6022, Train Loss: 1.3229\nEpoch 42: Train Accuracy: 0.6061, Train Loss: 1.3028\nEpoch 43: Train Accuracy: 0.6056, Train Loss: 1.3132\nEpoch 44: Train Accuracy: 0.6092, Train Loss: 1.3011\nEpoch 45: Train Accuracy: 0.6087, Train Loss: 1.2958\nEpoch 46: Train Accuracy: 0.6083, Train Loss: 1.2975\nEpoch 47: Train Accuracy: 0.6072, Train Loss: 1.2953\nEpoch 48: Train Accuracy: 0.6086, Train Loss: 1.2976\nEpoch 49: Train Accuracy: 0.6112, Train Loss: 1.2831\nEpoch 50: Train Accuracy: 0.6116, Train Loss: 1.2830\nEpoch 51: Train Accuracy: 0.6106, Train Loss: 1.2907\nEpoch 52: Train Accuracy: 0.6127, Train Loss: 1.2836\nEpoch 53: Train Accuracy: 0.6082, Train Loss: 1.2898\nEpoch 54: Train Accuracy: 0.6188, Train Loss: 1.2752\nEpoch 55: Train Accuracy: 0.6156, Train Loss: 1.2693\nEpoch 56: Train Accuracy: 0.6193, Train Loss: 1.2631\nEpoch 57: Train Accuracy: 0.6118, Train Loss: 1.2818\nEpoch 58: Train Accuracy: 0.6212, Train Loss: 1.2716\nEpoch 59: Train Accuracy: 0.6187, Train Loss: 1.2659\nEpoch 60: Train Accuracy: 0.6158, Train Loss: 1.2735\nEpoch 61: Train Accuracy: 0.6198, Train Loss: 1.2537\nEpoch 62: Train Accuracy: 0.6223, Train Loss: 1.2644\nEpoch 63: Train Accuracy: 0.6176, Train Loss: 1.2572\nEpoch 64: Train Accuracy: 0.6172, Train Loss: 1.2622\nEpoch 65: Train Accuracy: 0.6216, Train Loss: 1.2617\nEpoch 66: Train Accuracy: 0.6223, Train Loss: 1.2580\nEpoch 67: Train Accuracy: 0.6206, Train Loss: 1.2554\nEpoch 68: Train Accuracy: 0.6225, Train Loss: 1.2548\nEpoch 69: Train Accuracy: 0.6273, Train Loss: 1.2487\nEpoch 70: Train Accuracy: 0.6248, Train Loss: 1.2477\nEpoch 71: Train Accuracy: 0.6176, Train Loss: 1.2474\nEpoch 72: Train Accuracy: 0.6219, Train Loss: 1.2467\nEpoch 73: Train Accuracy: 0.6259, Train Loss: 1.2374\nEpoch 74: Train Accuracy: 0.6241, Train Loss: 1.2398\nEpoch 75: Train Accuracy: 0.6241, Train Loss: 1.2377\nEpoch 76: Train Accuracy: 0.6237, Train Loss: 1.2490\nEpoch 77: Train Accuracy: 0.6242, Train Loss: 1.2468\nEpoch 78: Train Accuracy: 0.6248, Train Loss: 1.2403\nEpoch 79: Train Accuracy: 0.6283, Train Loss: 1.2352\nEpoch 80: Train Accuracy: 0.6252, Train Loss: 1.2326\nModel trained successfully. Test accuracy: 0.6367\nTraining complete. Saving results to Excel...\nResults saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}